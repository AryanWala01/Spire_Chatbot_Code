{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fff75a83-ec0a-4848-b0ba-7c116a99add6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Run Sharepoint Access Script"
    }
   },
   "outputs": [],
   "source": [
    "#uncomment below cell for manual run else comment for job execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7b4a972-56ea-4f6d-8f26-261d6f676e06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./01-SharepointAccess_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc7cb011-7f72-4e2a-a9e9-e2c85a219761",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../Main_Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52897f07-e8b2-4867-9a51-b9ec1a22311f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../sql_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "489b99a4-ac23-4d58-8987-f1c466759eba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Comment this for manual run\n",
    "# %pip install -U -q -r ../requirements.txt\n",
    "# dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "562b299e-ee8c-4d62-b6f6-2d5ca8a36d96",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install -q \"camelot-py[all]\" --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f51f141a-ce47-4bf4-b980-036d819532ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Comment this for manual run\n",
    "# SRC_TASK_KEY = \"Getting_Sharepoint_Data\"\n",
    "\n",
    "# try:\n",
    "#     token = dbutils.jobs.taskValues.get(taskKey=SRC_TASK_KEY, key=\"access_token\", debugValue=None)\n",
    "#     site_id = dbutils.jobs.taskValues.get(taskKey=SRC_TASK_KEY, key=\"site_id\", debugValue=None)\n",
    "#     drive_id = dbutils.jobs.taskValues.get(taskKey=SRC_TASK_KEY, key=\"drive_id\", debugValue=None)\n",
    "# except Exception as e:\n",
    "#     print(\"Failed to fetch values from previous task:\", e)\n",
    "#     token = site_id = drive_id = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "268cd374-591d-4501-83e8-ce78263fe5bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Helper Functions to save file and to read content from the different file format.\n",
    "\n",
    "import camelot\n",
    "def save_file(content_response,volume_path,file_name):\n",
    "    if content_response.status_code == 200:\n",
    "        pdf_binary = content_response.content\n",
    "        # Define the Databricks volume path\n",
    "        volume_save_path = f\"{volume_path}\"+ f\"{file_name}\"\n",
    "        # Save the PDF to the volume\n",
    "        with open(volume_save_path, \"wb\") as f:\n",
    "            f.write(pdf_binary)\n",
    "            \n",
    "    else:\n",
    "        print(f\"Failed to download PDF. Status code: {content_response.status_code}\")\n",
    "        return False \n",
    "    # Verify the file exists\n",
    "    return volume_save_path\n",
    "\n",
    "def read_pdf_tables(file_path):\n",
    "    \"\"\"Reads tables from a PDF using Camelot.\"\"\"\n",
    "    tables = camelot.read_pdf(file_path, pages=\"all\", flavor=\"lattice\")\n",
    "    cleaned_tables = []\n",
    "    \n",
    "    for idx, table in enumerate(tables):\n",
    "        df = table.df.copy()\n",
    "\n",
    "        # Remove rows/columns that are completely empty\n",
    "        df.replace(r'^\\s*$', None, regex=True, inplace=True)\n",
    "        df.dropna(axis=0, how='all', inplace=True)\n",
    "        df.dropna(axis=1, how='all', inplace=True)\n",
    "\n",
    "        # Optional: skip tiny or invalid tables\n",
    "        if df.shape[0] < 2 or df.shape[1] < 2:\n",
    "            continue\n",
    "\n",
    "        # Clean line breaks and whitespace\n",
    "        df = df.applymap(lambda x: str(x).replace(\"\\n\", \" \").strip() if isinstance(x, str) else x)\n",
    "\n",
    "        cleaned_tables.append(df)\n",
    "    if not cleaned_tables:\n",
    "        return \"\"\n",
    "    return str(cleaned_tables)\n",
    "\n",
    "def read_docx_tables(file_path):\n",
    "    \"\"\"Reads tables from a Word document using python-docx.\"\"\"\n",
    "    doc = Document(file_path)\n",
    "    all_tables = []\n",
    "\n",
    "    for table in doc.tables:\n",
    "        data = []\n",
    "        for row in table.rows:\n",
    "            data.append([cell.text.strip() for cell in row.cells])\n",
    "        \n",
    "        df = pd.DataFrame(data)\n",
    "        \n",
    "        # Skip empty tables\n",
    "        if df.dropna(how='all').shape[0] > 0:\n",
    "            all_tables.append(df)\n",
    "\n",
    "    if not all_tables:\n",
    "        return \"\"\n",
    "    return str(all_tables)\n",
    "    \n",
    "def read_odt_tables(file_path):\n",
    "    \"\"\"Reads tables from an ODT file and returns a list of DataFrames.\"\"\"\n",
    "    odt_doc = load(file_path)\n",
    "    tables = odt_doc.getElementsByType(Table)\n",
    "    \n",
    "    table_dfs = []\n",
    "    for table in tables:\n",
    "        data = []\n",
    "        rows = table.getElementsByType(TableRow)\n",
    "        for row in rows:\n",
    "            row_data = []\n",
    "            cells = row.getElementsByType(TableCell)\n",
    "            for cell in cells:\n",
    "                texts = cell.getElementsByType(P)\n",
    "                text_content = \" \".join([str(t.firstChild.data if t.firstChild else \"\") for t in texts])\n",
    "                row_data.append(text_content.strip())\n",
    "            if any(cell.strip() for cell in row_data):  # Skip empty rows\n",
    "                data.append(row_data)\n",
    "        \n",
    "        if data:  # If table has content\n",
    "            df = pd.DataFrame(data)\n",
    "            table_dfs.append(df)\n",
    "\n",
    "    return str(table_dfs) if table_dfs else \"\"\n",
    "\n",
    "# Helper function to extract text from PDF\n",
    "def extract_text_from_pdf(pdf_content, file_name,volume_save_path):\n",
    "    try:\n",
    "        with pdfplumber.open(BytesIO(pdf_content)) as pdf:\n",
    "            text = \"\\n\".join([page.extract_text() for page in pdf.pages if page.extract_text()])\n",
    "        pdf_tables = read_pdf_tables(volume_save_path)\n",
    "        return text + pdf_tables\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error extracting text from PDF {file_name}: {e}\", exc_info=True)\n",
    "        raise RuntimeError(f\"Error extracting text from PDF {file_name}: {e}\") from e\n",
    "\n",
    "# Helper function to extract text from DOCX\n",
    "def extract_text_from_docx(docx_content, file_name,volume_save_path):\n",
    "    try:\n",
    "        doc = Document(BytesIO(docx_content))\n",
    "        doc_text = \"\\n\".join([para.text for para in doc.paragraphs]) \n",
    "        docx_tables = read_docx_tables(volume_save_path)\n",
    "        \n",
    "        return doc_text + docx_tables\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error extracting text from DOCX {file_name}: {e}\", exc_info=True)\n",
    "        raise RuntimeError(f\"Error extracting text from DOCX {file_name}: {e}\") from e\n",
    "\n",
    "# Helper function to extract text from XLSX\n",
    "def extract_text_from_xlsx(xlsx_content, file_name):\n",
    "    try:\n",
    "        df = pd.read_excel(BytesIO(xlsx_content), sheet_name=0)\n",
    "        return df  # Saving as Dataframe\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error extracting text from XLSX {file_name}: {e}\", exc_info=True)\n",
    "        raise RuntimeError(f\"Error extracting text from XLSX {file_name}: {e}\") from e\n",
    "\n",
    "# Helper function to extract text from ODT\n",
    "def extract_text_from_odt(odt_bytes, file_name,volume_save_path):\n",
    "    \"\"\"Read text content from raw bytes of an ODT file. Raises on failure.\"\"\"\n",
    "    try:\n",
    "        buffer = BytesIO(odt_bytes)\n",
    "        textdoc = load(buffer)\n",
    "\n",
    "        all_text = []\n",
    "        for elem in textdoc.getElementsByType(P):\n",
    "            if elem.firstChild:\n",
    "                all_text.append(str(elem.firstChild.data))\n",
    "\n",
    "        odt_tables = read_odt_tables(volume_save_path)\n",
    "        all_text_para = \"\\n\".join(all_text)\n",
    "        return all_text_para + odt_tables\n",
    "\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error extracting text from ODT {file_name}: {e}\") from e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c9ea4c4-2933-45ee-b1d1-673c2f1d0228",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Process and Extract Files from SharePoint Drive"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import pdfplumber\n",
    "from io import BytesIO\n",
    "from docx import Document\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType\n",
    "from odf.opendocument import load\n",
    "from odf.text import P \n",
    "from datetime import datetime\n",
    "\n",
    "def get_all_files():\n",
    "    try:\n",
    "        #uncomment for manual run\n",
    "        token = get_access_token()\n",
    "        site_id = get_site_id()\n",
    "        drive_id = get_drive_id()\n",
    "\n",
    "        volume_path = \"/Volumes/spire_catalog/default/temp_storage/\"\n",
    "        headers = {\"Authorization\": f\"Bearer {token}\"}\n",
    "        all_files = []\n",
    "\n",
    "        def process_folder(folder_id=None):\n",
    "            nonlocal headers, all_files\n",
    "\n",
    "            # Determine the URL based on whether it's the root or a subfolder\n",
    "            if folder_id is None:\n",
    "                url = microsoft_graph_url + f\"{site_id}\" + drive_path + f\"{drive_id}\" + root_path\n",
    "            else:\n",
    "                url = microsoft_graph_url + f\"{site_id}\" + drive_path + f\"{drive_id}\" + item_path + f\"{folder_id}\" + children_path\n",
    "\n",
    "            while url:\n",
    "                response = requests.get(url, headers=headers)\n",
    "                response.raise_for_status()\n",
    "                data = response.json()\n",
    "                items = data.get(\"value\", [])\n",
    "\n",
    "                for item in items:\n",
    "                    if 'folder' in item:\n",
    "                        process_folder(item['id'])  # Recursively process subfolders\n",
    "                    elif 'file' in item:\n",
    "                        file_location = item['webUrl']\n",
    "                        file_extension = file_location.lower().split('.')[-1][:4]\n",
    "                        file_id = item.get('id')\n",
    "                        file_name = item.get('name'),                                   \n",
    "                        created_datetime = datetime.strptime(item.get('createdDateTime'), \"%Y-%m-%dT%H:%M:%SZ\")\n",
    "                        last_modified_datetime = datetime.strptime(item.get('lastModifiedDateTime'), \"%Y-%m-%dT%H:%M:%SZ\")\n",
    "                        \n",
    "                        last_update_time = get_last_modified_datetime(source_table_name)\n",
    "                        \n",
    "                        if last_update_time is None:\n",
    "                            last_update_time = datetime(1900, 1, 1)\n",
    "\n",
    "                        if file_extension in FILE_LIST and last_modified_datetime > last_update_time:\n",
    "                            try:\n",
    "                                content_url = microsoft_graph_url + f\"{site_id}\" + drive_path + f\"{drive_id}\" + item_path + f\"{item['id']}\" + content_path\n",
    "                                \n",
    "                                content_response = requests.get(content_url, headers=headers)\n",
    "                                content_response.raise_for_status()\n",
    "                                \n",
    "                                #saving file to volume for reading tables\n",
    "                                volume_save_path = save_file(content_response,volume_path, item.get('name'))\n",
    "                                file_content = None\n",
    "\n",
    "                                if file_extension == \"txt\":\n",
    "                                    file_content = content_response.text\n",
    "                                \n",
    "                                elif file_extension == \"pdf\":\n",
    "                                    file_content = extract_text_from_pdf(content_response.content, file_name,volume_save_path)\n",
    "                                \n",
    "                                elif file_extension == \"docx\":\n",
    "                                    file_content = extract_text_from_docx(content_response.content, file_name,volume_save_path)\n",
    "\n",
    "                                elif file_extension == \"xlsx\" :\n",
    "                                    file_content = extract_text_from_xlsx(content_response.content, file_name)\n",
    "\n",
    "                                elif file_extension == \"odt&\" :\n",
    "                                    file_content=extract_text_from_odt(content_response.content, file_name,volume_save_path)\n",
    "\n",
    "                                all_files.append({\n",
    "                                    COL_FILE_ID: file_id,\n",
    "                                    COL_FILE_LOCATION: file_location,\n",
    "                                    COL_FILE_NAME: file_name[0],\n",
    "                                    COL_CONTENT: file_content,                                    \n",
    "                                    COL_CREATED_DATETIME: created_datetime,\n",
    "                                    COL_LAST_MODIFIED_DATETIME: last_modified_datetime,\n",
    "                                    COL_LOAD_DATETIME: datetime.utcnow()\n",
    "                                })\n",
    "                            \n",
    "                            except requests.exceptions.RequestException as e:\n",
    "                                logger.error(f\"Error reading file {file_location}: {e}\", exc_info=True)\n",
    "                                all_files.append({\n",
    "                                    COL_FILE_ID: file_id,\n",
    "                                    COL_FILE_LOCATION: file_location,\n",
    "                                    COL_FILE_NAME: file_name[0],\n",
    "                                    COL_CONTENT: None,\n",
    "                                    COL_CREATED_DATETIME: created_datetime,\n",
    "                                    COL_LAST_MODIFIED_DATETIME: last_modified_datetime,\n",
    "                                    COL_LOAD_DATETIME: datetime.utcnow()\n",
    "                                })\n",
    "\n",
    "                # Handle pagination\n",
    "                url = data.get('@odata.nextLink', None)\n",
    "\n",
    "        # Start processing from the root folder\n",
    "        process_folder()\n",
    "\n",
    "        # Convert to Spark DataFrame\n",
    "        schema = StructType([\n",
    "            StructField(COL_FILE_ID, StringType(), True),\n",
    "            StructField(COL_FILE_LOCATION, StringType(), True),\n",
    "            StructField(COL_FILE_NAME, StringType(), True),\n",
    "            StructField(COL_CONTENT, StringType(), True),\n",
    "            StructField(COL_CREATED_DATETIME, TimestampType(), True),\n",
    "            StructField(COL_LAST_MODIFIED_DATETIME, TimestampType(), True),\n",
    "            StructField(COL_LOAD_DATETIME, TimestampType(), True)\n",
    "        ])\n",
    "        \n",
    "        df = spark.createDataFrame(all_files, schema=schema)\n",
    "        return df\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logger.error(f\"Error fetching files: {e}\", exc_info=True)\n",
    "        return None\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee226701-95a5-494a-94bb-fc5ff4bc62d8",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Save DataFrame to Delta Table in Databricks"
    }
   },
   "outputs": [],
   "source": [
    "def save_to_table(df, database_name:str, table_name:str):\n",
    "  \n",
    "    table_path = f\"{default_catalog}.{database_name}.{table_name}\"\n",
    " \n",
    "    try:\n",
    "        df.write.format(\"delta\").mode(\"overwrite\").option(\"mergeSchema\", \"true\").saveAsTable(table_path)\n",
    "\n",
    "        logger.info(\"Table successfully saved.\")\n",
    " \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error saving table '{str(e)}\", exc_info=True)\n",
    " \n",
    "\n",
    "# Call function in Databricks notebook\n",
    "data_source = get_all_files()\n",
    "\n",
    "# Save DataFrame as a table\n",
    "if spark.catalog.tableExists(f\"{default_catalog}.{default_schema}.{source_table_name}\"):\n",
    "    try:\n",
    "        data_source.createOrReplaceTempView(TEMP_SOURCE_DATA)\n",
    "    \n",
    "        merge_with_table(source_table_name, TEMP_SOURCE_DATA)\n",
    "        # spark.sql(source_merge_stmt)\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error merging data: {str(e)}\", exc_info=True)\n",
    "else:\n",
    "    save_to_table(data_source, database_name = default_schema, table_name = source_table_name)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1086503001007927,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "02-SourceDataFromSharepoint_final",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
