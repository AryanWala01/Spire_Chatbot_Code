{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1ba3e6f3-6710-445d-abb9-25b2797281d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import traceback\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "class DeltaTableHandler(logging.Handler):\n",
    "    def __init__(self, table_name):\n",
    "        super().__init__()\n",
    "        self.table_name = table_name\n",
    "        self.spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "    def emit(self, record):\n",
    "        # Safely retrieve notebook context information\n",
    "        try:\n",
    "            notebook_path = dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get()\n",
    "        except Exception:\n",
    "            notebook_path = \"N/A\"\n",
    "        try:\n",
    "            job_id = dbutils.notebook.entry_point.getDbutils().notebook().getContext().jobId().get()\n",
    "        except Exception:\n",
    "            job_id = \"N/A\"\n",
    "        try:\n",
    "            run_id = dbutils.notebook.entry_point.getDbutils().notebook().getContext().runId().get()\n",
    "        except Exception:\n",
    "            run_id = \"N/A\"\n",
    "        try:\n",
    "            user = dbutils.notebook.entry_point.getDbutils().notebook().getContext().userName().get()\n",
    "        except Exception:\n",
    "            user = \"N/A\"\n",
    "\n",
    "        # Format exception information if present\n",
    "        if record.exc_info:\n",
    "            exception_text = ''.join(traceback.format_exception(*record.exc_info))\n",
    "        else:\n",
    "            exception_text = None\n",
    "\n",
    "        log_entry = {\n",
    "            \"timestamp\": datetime.utcnow().isoformat(),\n",
    "            \"level\": record.levelname,\n",
    "            \"message\": record.getMessage(),\n",
    "            \"notebook_name\": notebook_path,\n",
    "            \"job_id\": job_id,\n",
    "            \"run_id\": run_id,\n",
    "            \"user\": user,\n",
    "            \"function_name\": record.funcName,\n",
    "            \"line_number\": record.lineno,\n",
    "            \"exception\": exception_text\n",
    "        }\n",
    "\n",
    "        # Define the schema\n",
    "        schema = StructType([\n",
    "            StructField(\"timestamp\", StringType(), True),\n",
    "            StructField(\"level\", StringType(), True),\n",
    "            StructField(\"message\", StringType(), True),\n",
    "            StructField(\"notebook_name\", StringType(), True),\n",
    "            StructField(\"job_id\", StringType(), True),\n",
    "            StructField(\"run_id\", StringType(), True),\n",
    "            StructField(\"user\", StringType(), True),\n",
    "            StructField(\"function_name\", StringType(), True),\n",
    "            StructField(\"line_number\", IntegerType(), True),\n",
    "            StructField(\"exception\", StringType(), True)\n",
    "        ])\n",
    "\n",
    "        df = self.spark.createDataFrame([log_entry], schema=schema)\n",
    "        df.write.mode(\"append\").format(\"delta\").option(\"mergeSchema\", \"true\").saveAsTable(self.table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9b4824ca-f332-4e14-9250-4a4855763da6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Set up logger with the DeltaTableHandler\n",
    "def setup_logger(log_level=logging.INFO, delta_table_name=\"spire_catalog.logging.logs\"):\n",
    "    \"\"\"\n",
    "    Configure logger with DeltaTableHandler.\n",
    "    \n",
    "    Parameters:\n",
    "    - log_level: Logging level (default: INFO)\n",
    "    - delta_table_name: Name of the Delta table for logs (default: spire_catalog.logging.logs)\n",
    "    \n",
    "    Returns:\n",
    "    - Configured logger\n",
    "    \"\"\"\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logger.setLevel(log_level)\n",
    "    \n",
    "    # Add DeltaTableHandler for writing logs to Delta table\n",
    "    delta_handler = DeltaTableHandler(delta_table_name)\n",
    "    delta_handler.setLevel(log_level)\n",
    "    logger.addHandler(delta_handler)\n",
    "    \n",
    "    #Add console handler for immediate feedback\n",
    "    console_handler = logging.StreamHandler()\n",
    "    console_handler.setLevel(log_level)\n",
    "    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "    console_handler.setFormatter(formatter)\n",
    "    logger.addHandler(console_handler)\n",
    "    \n",
    "    return logger"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5592042140668703,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "DeltaTableHandler",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
