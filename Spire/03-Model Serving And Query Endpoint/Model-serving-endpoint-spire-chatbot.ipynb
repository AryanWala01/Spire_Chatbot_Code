{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c7d179b-e7e7-484c-b95a-feee88d18397",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install -U -q -r ../requirements.txt\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9890657d-3a05-4dab-ba50-efaa5d0bd903",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Initialize RAG Code with Optional Data Reset"
    }
   },
   "outputs": [],
   "source": [
    "%run ../00-init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65dffa1d-26df-49fb-a50f-66d52972be06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../sql_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "101b1818-5baf-4020-9507-1dc96d165145",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../Main_Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10fea4a7-23fd-4de0-9110-f59c61bb03ff",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Configure and Save RAG Chain Settings"
    }
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "import os\n",
    "import logging\n",
    "\n",
    "rag_chain_config = {\n",
    "    \"databricks_resources\": {\n",
    "        \"llm_endpoint_name\": \"databricks-meta-llama-3-3-70b-instruct\",\n",
    "        \"vector_search_endpoint_name\": VECTOR_SEARCH_ENDPOINT_NAME,\n",
    "        \"server_hostname\": dbutils.secrets.get(scope = \"DATABRICKS_CREDENTIALS\", key = \"server_hostname\"),\n",
    "        \"http_path\": dbutils.secrets.get(scope = \"DATABRICKS_CREDENTIALS\", key = \"http_path\"),\n",
    "        # \"access_token\": dbutils.secrets.get(scope = \"DATABRICKS_CREDENTIALS\", key = \"access_token\"),\n",
    "        \"sp_client_id\": dbutils.secrets.get(scope = \"DATABRICKS_CREDENTIALS\", key = \"sp_client_id\"),\n",
    "        \"sp_client_secret\": dbutils.secrets.get(scope = \"DATABRICKS_CREDENTIALS\", key = \"sp_client_secret\")\n",
    "    },\n",
    "    \"input_example\": {\n",
    "        \"messages\": [{\"content\": \"What is spire?\", \"role\": \"user\"}],\n",
    "        \"extra_params\": {\"user_id\": \"unknown.person@abc.com\"}\n",
    "    },\n",
    "    \"llm_config\": {\n",
    "        \"llm_parameters\": {\"max_tokens\": 1500, \"temperature\": 0.01},\n",
    "        \"llm_prompt_template\": \"\"\"You are an expert AI assistant. Your responses must follow these rules:\n",
    "General Behavior\n",
    "• As a AI assistant Rely only on the provided context and conversation history—do not hallucinate or introduce external information.\n",
    "• If the answer is not known from context,just day \"I couldn't find enough relevant context—could you please provide more details so I can give you an accurate response\".\n",
    "• You must handle both casual conversation and domain-specific queries gracefully.\n",
    "• When the user's question is vague, ambiguous, or lacks enough detail to  generate a helpful response, ask a clear and concise follow-up question to clarify their intent. Your goal is to help the user refine their question so you can provide an accurate and relevant answer.\n",
    "\n",
    "If the question is clear: answer it directly using the retrieved context.\n",
    "\n",
    "If the question is unclear or incomplete: respond with a short clarification like:\n",
    "\n",
    "\"Could you clarify what you're referring to with [ambiguous term]?\"\n",
    "\n",
    "\"Can you provide more context or specify what you're looking for?\"\n",
    "\n",
    "Conversation Handling\n",
    "• For casual messages like “Hello” or “How are you?”, respond warmly and naturally, like a friendly human.\n",
    "• If the user’s message is vague or ambiguous, ask a clear follow-up question rather than guessing.\n",
    "\n",
    "• If a user gives a follow-up question Follow these strict instructions:\n",
    "    1. The system will first attempt to retrieve context based only on the user's most recent question.\n",
    "    2. If no relevant information is found, it will then attempt retrieval using the full conversation history.\n",
    "    \n",
    "– If it’s clear, answer it directly.\n",
    "– If it’s unclear, ask a clarifying question before responding.\n",
    "• If the user asks a factual or task-based question, answer it clearly and concisely, based only on the provided context.\n",
    "• Never invent or assume information outside the context.\n",
    "\n",
    "Response Style\n",
    "Be Direct: Answer the question without restating it.\n",
    "\n",
    "No Fillers: Avoid phrases like “Here’s the answer” or “According to the context.”\n",
    "\n",
    "No Meta-Commentary: Don’t mention the prompt, context, or chat structure.\n",
    "\n",
    "Be Concise: Prefer one paragraph; only use more if necessary.\n",
    "\n",
    "Be Helpful: Always move the conversation forward with useful answers or clarifying questions.\n",
    "\n",
    "**last question**\n",
    "{last_question}\n",
    "\n",
    "**Conversation History:**  \n",
    "{chat_history}\n",
    "\n",
    "**Context:**  \n",
    "{context}\n",
    "\n",
    "**Question:**  \n",
    "{question}\n",
    "\"\"\",\n",
    "    \"llm_prompt_template_variables\": [\"context\", \"chat_history\", \"question\",\"last_question\"],\n",
    "    },\n",
    "    \"retriever_config\": {\n",
    "        \"chunk_template\": \"Passage: {chunk_text}\\n\",\n",
    "        \"data_pipeline_tag\": \"poc\",\n",
    "        \"parameters\": {\n",
    "            \"k\": 5,\n",
    "            \"query_type\": \"ann\"\n",
    "        },\n",
    "        \"schema\": {\n",
    "            \"chunk_text\": \"content\",\n",
    "            \"primary_key\": \"id\",\n",
    "            \"file_id\": \"file_id\"\n",
    "        },\n",
    "        \"vector_search_index\": \"spire_catalog.spire_schema.source_data_index\"\n",
    "    },\n",
    "    \"vector_embd_columns\": [\"content\", \"id\", \"file_id\"],\n",
    "    \"user_list_name\": [\"spire_demo_grp\"],\n",
    "    \"model_name\": \"spire_rag_final\",\n",
    "    \"catalog_name\": \"spire_catalog\",\n",
    "    \"schema_name\": \"spire_schema\",\n",
    "    \"source_data_table\": \"source_data_index\",\n",
    "    \"user_permission_table\": \"sharepoint_permissions\"\n",
    "}\n",
    "\n",
    "# Save configuration to file with error handling\n",
    "try:\n",
    "    with open('rag_chain_config.yaml', 'w') as f:\n",
    "        yaml.dump(rag_chain_config, f)\n",
    "    logger.info(\"Configuration saved successfully\")\n",
    "except Exception as e:\n",
    "    logger.warning(f\"Failed to save configuration to file: {str(e)}\")\n",
    "    logger.info(\"Continuing with in-memory configuration for build job\")\n",
    "\n",
    "model_config = mlflow.models.ModelConfig(development_config='rag_chain_config.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4480ce54-c8d7-4c48-b510-5a47ba3591f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%%writefile chain.py\n",
    "import os\n",
    "import time\n",
    "import logging\n",
    "import requests\n",
    "import pandas as pd\n",
    "import mlflow\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Dict, Any, Optional\n",
    "from operator import itemgetter\n",
    "\n",
    "from databricks.vector_search.client import VectorSearchClient\n",
    "from databricks import sql, sdk\n",
    "from databricks.sdk.core import Config, oauth_service_principal\n",
    "\n",
    "from langchain_community.chat_models import ChatDatabricks\n",
    "from langchain_community.vectorstores import DatabricksVectorSearch\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "from mlflow.models.rag_signatures import ChatCompletionRequest\n",
    "\n",
    "# ----------------------------\n",
    "#  Logging\n",
    "# ----------------------------\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s %(levelname)s %(name)s %(message)s\",\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "mlflow.langchain.autolog()\n",
    "\n",
    "model_config         = mlflow.models.ModelConfig(development_config=\"rag_chain_config.yaml\")\n",
    "databricks_resources = model_config.get(\"databricks_resources\")\n",
    "retriever_config     = model_config.get(\"retriever_config\")\n",
    "llm_config           = model_config.get(\"llm_config\")\n",
    "server_host_name    = databricks_resources[\"server_hostname\"]\n",
    "http_path           = databricks_resources[\"http_path\"]\n",
    "access_token_value  = databricks_resources[\"access_token\"]\n",
    "llm_endpoint = databricks_resources[\"llm_endpoint_name\"]\n",
    "\n",
    "input_example     = model_config.get(\"input_example\")\n",
    "\n",
    "# ----------------------------\n",
    "#  Custom Chat Request Schema\n",
    "\n",
    "# ----------------------------\n",
    "def connect_to_databricks(server_hostname: str, http_path: str, sp_client_id: str, sp_client_secret:str):\n",
    "\n",
    "    def credential_provider():\n",
    "        config = Config(\n",
    "            host          = f\"https://{server_hostname}\",\n",
    "            client_id     = sp_client_id,\n",
    "            client_secret =  sp_client_secret)\n",
    "        return oauth_service_principal(config)\n",
    "\n",
    "    return sql.connect(\n",
    "        server_hostname=server_hostname,\n",
    "        http_path=http_path,\n",
    "        credentials_provider=credential_provider,\n",
    "    )\n",
    "\n",
    "def extract_file_id(user_id: str) -> List[str]:\n",
    "    server = databricks_resources[\"server_hostname\"]\n",
    "    path   = databricks_resources[\"http_path\"]\n",
    "    sp_client_id = databricks_resources[\"sp_client_id\"]\n",
    "    sp_client_secret = databricks_resources[\"sp_client_secret\"]\n",
    "    conn   = connect_to_databricks(server, path, sp_client_id, sp_client_secret)\n",
    "    query  = (f\"\"\"SELECT file_id FROM spire_catalog.spire_schema.sharepoint_permissions WHERE user_id = '{user_id}'\"\"\")\n",
    "\n",
    "    df = pd.read_sql(query, conn)\n",
    "    conn.close()\n",
    "    ids = df[\"file_id\"].drop_duplicates().tolist()\n",
    "    if not ids:\n",
    "        logger.warning(f\"No file permissions for user {user_id}\")\n",
    "    return ids\n",
    "\n",
    "# ----------------------------\n",
    "def combine_all_messages(chat_messages: List[Dict[str, str]]) -> str:\n",
    "    return extract_user_query_string(chat_messages)\n",
    "\n",
    "def extract_user_query_string(chat_messages: List[Dict[str, str]]) -> str:\n",
    "    return chat_messages[-1][\"content\"] \n",
    "\n",
    "\n",
    "def extract_previous_messages(chat_messages_array, assistant_response=None):\n",
    "    messages = \"\"\n",
    "    for msg in chat_messages_array:\n",
    "        messages += f\"\\n{msg['role']}: {msg['content']}\"\n",
    "    if assistant_response:\n",
    "        messages += f\"  assistant: {assistant_response}\\n\"\n",
    "    return messages\n",
    "\n",
    "\n",
    "def store_response_with_history(inputs_and_response: dict):\n",
    "    chat_messages = inputs_and_response[\"inputs\"][\"messages\"]\n",
    "    model_response = inputs_and_response[\"output\"]\n",
    "    \n",
    "    updated_history = extract_previous_messages(chat_messages, assistant_response=model_response)\n",
    "    \n",
    "    return model_response \n",
    "\n",
    "def format_context(docs: List[Any]) -> str:\n",
    "    if not docs:\n",
    "        return \"No relevant documents found.\"\n",
    "    template = retriever_config[\"chunk_template\"]\n",
    "    return \"\".join(template.format(chunk_text=d.page_content) for d in docs)\n",
    "\n",
    "def most_recent_message(chat_messages_array):\n",
    "    if len(chat_messages_array) > 1:\n",
    "        return chat_messages_array[-2][\"content\"]\n",
    "    return chat_messages_array[-1][\"content\"]\n",
    "\n",
    "def get_vector_search_index(retries: int = 3, delay: float = 2.0):\n",
    "    client = VectorSearchClient(\n",
    "        workspace_url=f\"https://{databricks_resources['server_hostname']}\",\n",
    "        service_principal_client_id=databricks_resources['sp_client_id'], \n",
    "        service_principal_client_secret=databricks_resources['sp_client_secret'],\n",
    "        disable_notice=True,\n",
    "    )\n",
    "    for i in range(retries):\n",
    "        try:\n",
    "            return client.get_index(\n",
    "                endpoint_name=databricks_resources[\"vector_search_endpoint_name\"],\n",
    "                index_name=retriever_config[\"vector_search_index\"],\n",
    "            )\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Index fetch failed (attempt {i+1}): {e}\")\n",
    "            time.sleep(delay)\n",
    "    raise RuntimeError(\"Could not retrieve vector search index\")\n",
    "\n",
    "vs_index = get_vector_search_index()\n",
    "schema   = retriever_config[\"schema\"]\n",
    "\n",
    "# ----------------------------\n",
    "#  Retriever with embedded user_id\n",
    "# ----------------------------\n",
    "def dynamic_retriever_with_user(inputs: Dict[str, Any]) -> List[Any]:\n",
    "    \"\"\"\n",
    "    inputs is a dict with keys:\n",
    "      - \"messages\": List[{\"role\":.., \"content\":..}]\n",
    "      - \"custom_inputs\": {\"user_id\":.., \"filters\":{..}}\n",
    "    \"\"\"\n",
    "    chat_messages = inputs[\"messages\"]\n",
    "    user_id       = inputs[\"custom_inputs\"][\"user_id\"]\n",
    "\n",
    "    # lookup file_ids\n",
    "    file_ids = extract_file_id(user_id)\n",
    "\n",
    "    # build search kwargs\n",
    "    params = retriever_config[\"parameters\"].copy()\n",
    "    params[\"filters\"] = (\n",
    "        {\"file_id\": file_ids}\n",
    "        if file_ids\n",
    "        else {\"file_id\": [\"__no_permission__\"]}\n",
    "    )\n",
    "\n",
    "    retr = DatabricksVectorSearch(\n",
    "        vs_index,\n",
    "        text_column=schema[\"chunk_text\"],\n",
    "        columns=[\n",
    "            schema[\"primary_key\"],\n",
    "            schema[\"chunk_text\"],\n",
    "            schema[\"file_id\"],\n",
    "        ],\n",
    "    ).as_retriever(search_kwargs=params)\n",
    "\n",
    "    query = combine_all_messages(chat_messages)\n",
    "    return retr.get_relevant_documents(query)\n",
    "\n",
    "runnable_retriever = RunnableLambda(dynamic_retriever_with_user)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=llm_config[\"llm_prompt_template\"],\n",
    "    input_variables=[\"chat_history\", \"context\", \"question\"],\n",
    ")\n",
    "\n",
    "model = ChatDatabricks(\n",
    "    endpoint=databricks_resources[\"llm_endpoint_name\"],\n",
    "    extra_params=llm_config[\"llm_parameters\"],\n",
    ")\n",
    "\n",
    "core_chain = (\n",
    "    {\n",
    "        \"context\": (\n",
    "            {\"messages\": itemgetter(\"messages\"), \"custom_inputs\": itemgetter(\"extra_params\")}\n",
    "            | runnable_retriever\n",
    "            | RunnableLambda(format_context)\n",
    "        ),\n",
    "        # Extract the question string\n",
    "        \"question\": itemgetter(\"messages\") | RunnableLambda(extract_user_query_string),\n",
    "        # Extract chat history (everything except last message)\n",
    "        \"chat_history\": itemgetter(\"messages\") | RunnableLambda(extract_previous_messages),\n",
    "        \"last_question\": itemgetter(\"messages\") | RunnableLambda(most_recent_message),\n",
    "    }\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "chain = RunnableLambda(lambda inputs: {\"inputs\": inputs, \"output\": core_chain.invoke(inputs)}) | RunnableLambda(store_response_with_history)\n",
    "\n",
    "\n",
    "mlflow.models.set_retriever_schema(\n",
    "    primary_key=schema[\"primary_key\"],\n",
    "    text_column=schema[\"chunk_text\"],\n",
    ")\n",
    "mlflow.models.set_model(model=chain)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "229d7b1f-8f2c-43d4-bff9-b9214f9c80c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "input_example = {\n",
    "        \"messages\": [{\"content\": \"What is spire?\", \"role\": \"user\"}],\n",
    "        \"extra_params\":{\"user_id\": \"aanchal.gupta@lirik.io\"}\n",
    "}\n",
    "with mlflow.start_run(run_name=f\"demo_rag_quickstart\"):\n",
    "    logged_chain_info = mlflow.langchain.log_model(\n",
    "        lc_model=os.path.join(os.getcwd(), 'chain.py'),\n",
    "        model_config='rag_chain_config.yaml',  # Chain configuration \n",
    "        artifact_path=\"chain\",  # Required by MLflow\n",
    "        input_example=input_example,  \n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23dfd8c3-febb-444d-adab-ab51b9954e86",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "chain = mlflow.langchain.load_model(logged_chain_info.model_uri)\n",
    "chain.invoke({\n",
    "        \"messages\": [{\"content\": \"What is spire?\", \"role\": \"user\"}],\n",
    "        \"extra_params\":{\"user_id\": \"aanchal.gupta@lirik.io\"}\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4ebcaf6-34ce-4875-a56a-d80e07ffa584",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks import agents\n",
    "MODEL_NAME = model_config.get(\"model_name\")\n",
    "MODEL_NAME_FQN = f\"{catalog}.{db}.{MODEL_NAME}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44b9c88d-a4d2-4414-a501-bd977a96be23",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Register the chain to UC\n",
    "uc_registered_model_info = mlflow.register_model(model_uri=logged_chain_info.model_uri, name=MODEL_NAME_FQN)\n",
    "\n",
    "# Deploy to enable the Review APP and create an API endpoint\n",
    "deployment_info = agents.deploy(model_name=MODEL_NAME_FQN, model_version=uc_registered_model_info.version, scale_to_zero=True)\n",
    "\n",
    "wait_for_model_serving_endpoint_to_be_ready(deployment_info.endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b01caae-8ca0-4b0a-a1f4-4cd17e42cc2b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "user_list = user_list_name\n",
    "\n",
    "agents.set_permissions(model_name=MODEL_NAME_FQN, users=user_list, permission_level=agents.PermissionLevel.CAN_QUERY)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5040160480823365,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Model-serving-endpoint-spire-chatbot",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
