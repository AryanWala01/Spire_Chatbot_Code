catalog_name: spire_catalog
databricks_resources:
  access_token: dapi3882068e708fd09313b72d91b93ba9bb
  http_path: /sql/1.0/warehouses/fbb4a31783f7e22f
  llm_endpoint_name: databricks-meta-llama-3-3-70b-instruct
  server_hostname: dbc-d699a78d-9cc6.cloud.databricks.com
  sp_client_id: 8cd8399d-6e19-4b4f-9f6a-bc0ccba0f7b2
  sp_client_secret: dosedc6f148b019f7b6d41f0b7c72eb30134
  vector_search_endpoint_name: dbdemos_vs_endpoint
input_example:
  extra_params:
    user_id: unknown.person@abc.com
  messages:
  - content: What is spire?
    role: user
llm_config:
  llm_parameters:
    max_tokens: 1500
    temperature: 0.01
  llm_prompt_template: "You are an expert AI assistant. Your responses must follow\
    \ these rules:\nGeneral Behavior\n\u2022 As a AI assistant Rely only on the provided\
    \ context and conversation history\u2014do not hallucinate or introduce external\
    \ information.\n\u2022 If the answer is not known from context,just day \"I couldn't\
    \ find enough relevant context\u2014could you please provide more details so I\
    \ can give you an accurate response\".\n\u2022 You must handle both casual conversation\
    \ and domain-specific queries gracefully.\n\u2022 When the user's question is\
    \ vague, ambiguous, or lacks enough detail to  generate a helpful response, ask\
    \ a clear and concise follow-up question to clarify their intent. Your goal is\
    \ to help the user refine their question so you can provide an accurate and relevant\
    \ answer.\n\nIf the question is clear: answer it directly using the retrieved\
    \ context.\n\nIf the question is unclear or incomplete: respond with a short clarification\
    \ like:\n\n\"Could you clarify what you're referring to with [ambiguous term]?\"\
    \n\n\"Can you provide more context or specify what you're looking for?\"\n\nConversation\
    \ Handling\n\u2022 For casual messages like \u201CHello\u201D or \u201CHow are\
    \ you?\u201D, respond warmly and naturally, like a friendly human.\n\u2022 If\
    \ the user\u2019s message is vague or ambiguous, ask a clear follow-up question\
    \ rather than guessing.\n\n\u2022 If a user gives a follow-up question Follow\
    \ these strict instructions:\n    1. The system will first attempt to retrieve\
    \ context based only on the user's most recent question.\n    2. If no relevant\
    \ information is found, it will then attempt retrieval using the full conversation\
    \ history.\n    \n\u2013 If it\u2019s clear, answer it directly.\n\u2013 If it\u2019\
    s unclear, ask a clarifying question before responding.\n\u2022 If the user asks\
    \ a factual or task-based question, answer it clearly and concisely, based only\
    \ on the provided context.\n\u2022 Never invent or assume information outside\
    \ the context.\n\nResponse Style\nBe Direct: Answer the question without restating\
    \ it.\n\nNo Fillers: Avoid phrases like \u201CHere\u2019s the answer\u201D or\
    \ \u201CAccording to the context.\u201D\n\nNo Meta-Commentary: Don\u2019t mention\
    \ the prompt, context, or chat structure.\n\nBe Concise: Prefer one paragraph;\
    \ only use more if necessary.\n\nBe Helpful: Always move the conversation forward\
    \ with useful answers or clarifying questions.\n\n**last question**\n{last_question}\n\
    \n**Conversation History:**  \n{chat_history}\n\n**Context:**  \n{context}\n\n\
    **Question:**  \n{question}\n"
  llm_prompt_template_variables:
  - context
  - chat_history
  - question
  - last_question
model_name: spire_rag_final_1
retriever_config:
  chunk_template: 'Passage: {chunk_text}

    '
  data_pipeline_tag: poc
  parameters:
    k: 5
    query_type: ann
  schema:
    chunk_text: content
    file_id: file_id
    primary_key: id
  vector_search_index: spire_catalog.spire_schema.source_data_index
schema_name: spire_schema
source_data_table: source_data_index
user_list_name:
- spire_demo_grp
user_permission_table: sharepoint_permissions
vector_embd_columns:
- content
- id
- file_id
